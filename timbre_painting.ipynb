{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "timbre_painting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YLyiTwPfVCT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosheman5/timbre_painting/blob/master/timbre_painting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "##### Inspired by https://github.com/magenta/ddsp/blob/master/ddsp/colab/demos/timbre_transfer.ipynb\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bvp6GWqtfVCW",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JndnmDMp66FL"
      },
      "source": [
        "# Hierarchical Timbre-Painting and Articulation Generation Timbre Transfer \n",
        "\n",
        "This notebook is a timbre transfer application using articulation and hierarchical timbre-painting, as detailed in our paper:\n",
        "* [Paper](https://arxiv.org/abs/2008.13095)\n",
        "* [Audio Examples](https://mosheman5.github.io/timbre_painting/) \n",
        "\n",
        "The notebook extracts loudness and pitch features from a given audio sample, uploaded or recoreded using a microphone.\n",
        "\n",
        "You can choose using pretrained models or upload your own trained model.\n",
        "\n",
        "### Instructions for running:\n",
        "\n",
        "* Make sure to use a GPU runtime, click:  __Runtime >> Change Runtime Type >> GPU__\n",
        "* Press ▶️ on the left of each of the cells\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "6wZde6CBya9k",
        "colab": {}
      },
      "source": [
        "#@title #Initialization\n",
        "\n",
        "#@markdown Clone the repository and install packages\n",
        "!git clone https://github.com/mosheman5/timbre_painting\n",
        "%cd timbre_painting\n",
        "!pip install hydra-core==0.11.3\n",
        "!pip install torchaudio\n",
        "!pip install soundfile\n",
        "!pip install -qU ddsp\n",
        "# # Ignore a bunch of deprecation warnings\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9iS48Ieg3z5",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@markdown Handle imports\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "from utils.utils import (create_srs, BaseAudio, load_audio, \n",
        "                         PRETRAINED_MODEL_DICT, download_pretrained_model)\n",
        "from utils.sampling import resample_torch, create_samplers\n",
        "from models.networks import ParallelWaveGANGenerator\n",
        "from timbre_painting import (load_norm_dicts, shift_ld, norm_loudness, \n",
        "                             f0_transfer, load_trained_pyramid, calc_loudness_list)\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from data_utils.spectral_feats import calc_loudness\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from ddsp.colab.colab_utils import (\n",
        "    auto_tune, detect_notes, fit_quantile_transform, \n",
        "    get_tuning_factor, download, play, record, \n",
        "    specplot, upload, DEFAULT_SAMPLE_RATE)\n",
        "import time\n",
        "from google.colab import files\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Helper Functions\n",
        "SAMPLE_RATE = DEFAULT_SAMPLE_RATE  # 16000\n",
        "\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "Go36QW9AS_CD",
        "colab": {}
      },
      "source": [
        "#@title Record or Upload Audio\n",
        "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
        "#@markdown * Audio should be monophonic (single instrument / voice)\n",
        "\n",
        "record_or_upload = \"Record\"  #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "\n",
        "record_seconds =     10#@param {type:\"number\", min:1, max:10, step:1}\n",
        "\n",
        "if record_or_upload == \"Record\":\n",
        "  audio = record(seconds=record_seconds+2)\n",
        "  audio = audio[SAMPLE_RATE*2:]\n",
        "else:\n",
        "  # Load audio sample here (.mp3 or .wav3 file)\n",
        "  # Just use the first file.\n",
        "  filenames, audios = upload()\n",
        "  audio = audios[0]\n",
        "audio = audio[np.newaxis, :]\n",
        "print('\\nExtracting audio features...')\n",
        "\n",
        "# Plot.\n",
        "specplot(audio)\n",
        "play(audio)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "wmSGDWM5yyjm",
        "colab": {}
      },
      "source": [
        "#@title Load a model\n",
        "#@markdown Load pretrained model/user's model, file processors and initializes weights\n",
        "model = 'Violin_v2' #@param ['Trumpet', 'Violin', 'Violin_v2', 'Saxophone', 'Cello', 'Upload your own (checkpoint folder as .tar.gz)']\n",
        "MODEL = model\n",
        "\n",
        "def find_model_dir(dir_name):\n",
        "  # Iterate through directories until model directory is found\n",
        "  for root, dirs, filenames in os.walk(dir_name):\n",
        "    for filename in filenames:\n",
        "      if filename.endswith(\"args.pth\") and not filename.startswith(\".\"):\n",
        "        model_dir = root\n",
        "        break\n",
        "  return model_dir \n",
        "\n",
        "\n",
        "if model in ('Trumpet', 'Violin', 'Violin_v2', 'Saxophone', 'Cello',):\n",
        "  # Pretrained models.\n",
        "  PRETRAINED_DIR = '/content/pretrained'\n",
        "  # remove old checkpoints and download model\n",
        "  !rm -r $PRETRAINED_DIR &> /dev/null\n",
        "  !mkdir $PRETRAINED_DIR &> /dev/null\n",
        "  model_path = download_pretrained_model(model, PRETRAINED_DIR)\n",
        "  \n",
        "else:\n",
        "  # User's model.\n",
        "  UPLOAD_DIR = '/content/uploaded'\n",
        "  !rm -r $UPLOAD_DIR &> /dev/null\n",
        "  !mkdir $UPLOAD_DIR\n",
        "  uploaded_files = files.upload()\n",
        "\n",
        "  for fnames in uploaded_files.keys():\n",
        "    print(\"Extracting... {}\".format(fnames))\n",
        "    !tar -xzvf $fnames -C $UPLOAD_DIR &> /dev/null\n",
        "  model_path = find_model_dir(UPLOAD_DIR)\n",
        "\n",
        "model_path = Path(model_path)\n",
        "\n",
        "run_args = torch.load(model_path / 'args.pth')\n",
        "\n",
        "# define args from trained model\n",
        "sr = run_args.sr\n",
        "num_scales = run_args.num_scales\n",
        "scale_factor = run_args.scale_factor\n",
        "max_value = run_args.max_val\n",
        "max_value_f0 = run_args.max_val_f0\n",
        "cond_freq = run_args.cond_freq\n",
        "\n",
        "# Pytorch device\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "#load processors\n",
        "crepe_path = 'data_utils/crepe_models/full.pth'\n",
        "base_audio = BaseAudio(crepe_path, device, True)\n",
        "srs = create_srs(sr, num_scales, scale_factor)\n",
        "samplers = create_samplers(srs, device=device)\n",
        "norm_dicts = load_norm_dicts(model_path / 'loudness.json')\n",
        "\n",
        "#load model\n",
        "Gs = load_trained_pyramid(model_path, network_params=run_args.generator_params, device=device, srs=srs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "uQFUlIJ_5r36",
        "colab": {}
      },
      "source": [
        "#@title Modify conditioning\n",
        "\n",
        "#@markdown Please tune the octave shift to be aligned with the target instrument. Trial and error are recommended, to see which key fits best your audio and to have some fun along the way!\n",
        "\n",
        "#@markdown Shift the pitch (octaves)\n",
        "pitch_shift =  0 #@param {type:\"slider\", min:-3, max:3, step:0.5}\n",
        "\n",
        "#@markdown Shift the loudness (octaves)\n",
        "loudness_shift =  0 #@param {type:\"slider\", min:-30, max:30, step:5}\n",
        "\n",
        "#@markdown Zero out unvoiced pitch, sometimes removes original signal\n",
        "zero_unvoiced_pitch = False #@param{type:\"boolean\"}\n",
        "\n",
        "def norm_loudness_list(loudness_list, loudness_shift):\n",
        "  for it, loudness_item in enumerate(loudness_list):\n",
        "    loudness_list[it] = loudness_item + loudness_shift\n",
        "  return loudness_list\n",
        "\n",
        "real_audio = audio.squeeze()\n",
        "loudness_hop = 8 * sr // cond_freq\n",
        "real_audio = real_audio[:len(real_audio) // loudness_hop * loudness_hop]\n",
        "loudness_list = calc_loudness_list(audio=real_audio, srs=srs, device=device,\n",
        "                                    sr_in=sr, norm_dicts=norm_dicts)\n",
        "loudness_list = norm_loudness_list(loudness_list, loudness_shift)\n",
        "real_audio, _, frequency = base_audio.forward(real_audio, sr, max_value_f0, \n",
        "                              numpy_flag=True, octave=2**pitch_shift, \n",
        "                              return_raw=True, unvoiced_flag=zero_unvoiced_pitch)\n",
        "\n",
        "real_audio_orig = real_audio[None, None, ...].to(device)\n",
        "# resample input to the wanted scale\n",
        "real_audio = resample_torch(real_audio_orig, sr, srs[0], max_val=max_value_f0)\n",
        "\n",
        "\n",
        "# Plot Features.\n",
        "n_plots = 2\n",
        "fig, axes = plt.subplots(nrows=n_plots, \n",
        "                      ncols=1, \n",
        "                      sharex=False,\n",
        "                      figsize=(8, 2*n_plots))\n",
        "ax = axes[0]\n",
        "ax.plot(librosa.hz_to_midi(frequency / 2**pitch_shift))\n",
        "ax.plot(librosa.hz_to_midi(frequency))\n",
        "ax.set_ylabel('f0 [midi]')\n",
        "_ = ax.legend(['Original','Adjusted'])\n",
        "\n",
        "ax = axes[1]\n",
        "loudness_plot = loudness_list[-1].squeeze().cpu().numpy()\n",
        "ax.plot(loudness_plot)\n",
        "ax.plot(loudness_plot + loudness_shift)\n",
        "ax.set_ylabel('Loudness [dB]')\n",
        "_ = ax.legend(['Original','Adjusted'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "SLwg1WkHCXQO",
        "colab": {}
      },
      "source": [
        "#@title #Resynthesize Audio\n",
        "\n",
        "# Run a batch of predictions.\n",
        "start_time = time.time()\n",
        "audio_outputs = f0_transfer(real_audio,loudness_list, Gs, samplers, max_val=max_value, save_all = False)\n",
        "audio_gen=audio_outputs[0]\n",
        "audio_gen = audio_gen.squeeze(0).cpu().numpy()\n",
        "audio_gen *= (0.4 / abs(audio_gen).max())\n",
        "print('Generation took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "# Plot\n",
        "print('Original')\n",
        "play(audio)\n",
        "\n",
        "print('Resynthesis')\n",
        "play(audio_gen)\n",
        "\n",
        "specplot(audio)\n",
        "plt.title(\"Original\")\n",
        "\n",
        "specplot(audio_gen)\n",
        "_ = plt.title(\"Resynthesis\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}